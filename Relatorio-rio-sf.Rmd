---
title: "Trabalho de Aplicação - Rio São Franscisco"
author: 'Yugo Oyama NUSP: 9297784'
date: "14/11/2021"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Introdução

Este trabalho consiste na aplicação de técnicas estatísticas abordadas no curso de Estatística em Altas Dimensões - 2021 em dois bancos de dados que serão explicados detalhadamente mais a frente.

## Chuva e vazão no Rio São Francisco

### Dados

Os dados armazenados no arquivo dados_rio_sf.rdata são referentes a medições de vazão e chuva em estações localizadas na região do rio São Francisco.

```{r}
load("dados_rio_sf.rdata")
```

Dentro deste arquivo, há três variáveis:

treino_sf: conjunto de treino com as medições de vazão e precipitação nas estações consideradas;
estacoes: com informações sobre as estações que coletam os dados;
teste_sf: conjundo com dados que serão usados para avaliar a performance dos modelos preditivos;


### Dados de treinamento

O dataframe dados_treino contem 1717 linhas e 83 colunas. Cada linha contêm medições semanais de vazão em diferentes estações fluviométricas sobre o rio São Francisco e de chuva em estações pluviométricas em regiões próximas do rio. Se a estação é fluviométrica, o dado armazenado é a vazão média registrada na semana correspondente. Caso a estação seja pluviométrica, o dado registrado é a chuva acumulada naquela semana. A primeira coluna, chamada Y, contém a medida de vazão na estação fluviométrica de código 46998000, que corresponde à estação em que se tem interesse em fazer previsão. A vazão na coluna Y corresponde à medição na semana seguinte às demais medições da mesma linha. 

### Estações

O dataframe estações contém informações sobre cada uma das estações que aparecem no dataframe treino_sf. Essas informações estão disponibilizadas apenas a título de curiosidade e não devem ser utilizadas para construir os modelos preditivos. Por exemplo, latitude e longitude, além do tipo da estação (fluviométrica ou pluviométrica) e outros detalhes técnicos de cada uma delas.

### Objetivo

O objetivo desta análise é propor um modelo que consiga realizar boas predições para as medições de vazão na estação 46998000 (coluna Y), dadas as medições tomadas nas estações do sistema na semana anterior (demais colunas). 

### Técnicas

Para esse projeto serão usados modelos lineares com regularização, modelos baseados em árvores, e redes neurais.  

Para comparar os modelos entre si,será utilizado o erro absoluto médio (MAE) e ao final de cada método testado, será adicionado o resultado a uma tabela comparativa.



```{r, results=FALSE, warning=FALSE, message=FALSE}
library(glmnet) # tem as funcoes para lasso, ridge e elasticnet
library(dplyr) # manipular tabelas e banco de dados
library(keras) # redes neurais
library(randomForest) # funcoes para estimar floresta aleatoria

```

### Parâmetros

Para o ajuste do modelo, foi definido que o conjunto de treino seria dividido em conjunto de treino e validação na proporção 8:2, ou seja, 20% do conjunto originalmente de treino foi usado para validação. Foi definida também uma semente por questão de reprodutibilidade (1234).

```{r}
# Definindo semente e porcentagem da amostra de treino e de validação.
set.seed(1234)

# separando 75% dos dados para treino
ids <- sample(nrow(treino_sf), size = .80*nrow(treino_sf), replace = FALSE) 

```


```{r}
# X matriz com variaveis preditoras (-1 para retirar o intercepto)
X <- model.matrix(Y ~ .,
                  data = treino_sf)[,-1]
  

```

### Modelo Média

De forma a ter uma base para comparar, foi construído um modelo simples que considera todos os valores da estação 46998000, calcula a média e sempre preve que a próxima medição será a média.

Temos os resultados abaixo.

```{r}
media <- mean(X[,"`46998000`"])
y_media_dentro <- rep(media,nrow(X[ids,]))

y_media_fora <- rep(media,nrow(X[-ids,]))

media_erro_dentro <- mean(abs(y_media_dentro - treino_sf$Y[ids]))

media_erro_fora <- mean(abs((y_media_fora - treino_sf$Y[-ids])))

```

```{r}
resultados <- data.frame("Media",media_erro_dentro, media_erro_fora)
names(resultados) <- c("modelo", "erro_dentro", "erro_fora")

resultados <- resultados %>% mutate(across(where(is.numeric),round,4))
resultados

```

<!-- ### Modelo Linear -->
<!-- ```{r} -->
<!-- reg_simples<- lm(Y~., data = treino_sf[ids,]) -->

<!-- ``` -->

<!-- ```{r} -->

<!-- y_lm_dentro <- predict(reg_simples, newx = X[ids,]) # valor predito dentro da amostra -->

<!-- y_lm_fora <- predict(reg_simples, newx = X[-ids,]) # valor predito fora da amostra -->
<!-- ``` -->


<!-- ```{r} -->
<!-- (lasso_erro_dentro <- mean(abs(y_lm_dentro - treino_sf$Y[ids]))) -->

<!-- (lasso_erro_fora <- mean(abs(y_lm_fora - treino_sf$Y[-ids]))) -->
<!-- ``` -->

### Lasso

Para criar um modelo de regressão com a penalização lasso, foi utilizada a biblioteca glmnet. Com ela, dentro da amostra de treino, foi ajustado o modelo de regressão com penalização lasso utizando validação cruzada. Em seguida, foi testado o modelo obtido no conjunto de validação e calculado o respectivo erro dentro e fora.

Para a escolha do lambda, iniciou-se uma sequência: 0.01, 0.1, 1, 2, 10, 25, 50, 75, 100 e permitiu que a função cv.glmnet escolhesse o melhor lambda. Existem dois lambdas que são popularmentes usados: o que minimiza o erro gerado pela validação cruzada e o no qual o erro não ultrapassa um desvio padrão do melhor modelo. Com isso, foram testados modelos com cada um dos lambdas e calculados os erros dentro e fora respectivos de cada um.

Ainda como modelo alternativo, foi definido que a função cv.glmnet escolhesse 30 lambdas diferentes e ajustasse um modelo.

Os resultados obtidos podem ser observados na tabela a seguir.

```{r, eval=FALSE}
cv_lasso <- cv.glmnet(X[ids,], treino_sf$Y[ids], alpha = 1,
                      lambda = c(0.01, 0.1, 1, 2, 10, 25, 50, 75, 100))
cv_lasso2 <- cv.glmnet(X[ids,], treino_sf$Y[ids], alpha = 1,
                      nlambda=30)
# cv_lasso <- cv.glmnet(X[ids,], treino_sf$Y[ids], alpha = 1,
#                       nlambda=500)

# cv_lasso$lambda.min
```

```{r, eval=FALSE}
saveRDS(cv_lasso, "sf_cv_lasso.rds")
saveRDS(cv_lasso2, "sf_cv_lasso2.rds")
```

```{r, eval=TRUE}
cv_lasso <- readRDS("sf_cv_lasso.rds")
cv_lasso2 <- readRDS("sf_cv_lasso2.rds")
```

```{r}
y_lasso_dentro <- predict(cv_lasso, newx = X[ids,],
                          s = cv_lasso$lambda.min) # valor predito dentro da amostra
y_lasso_dentro1.2 <- predict(cv_lasso, newx = X[ids,],
                          s = cv_lasso$lambda.1se) # valor predito dentro da amostra

y_lasso_fora <- predict(cv_lasso, newx = X[-ids,],
                          s = cv_lasso$lambda.min) # valor predito fora da amostra
y_lasso_fora1.2 <- predict(cv_lasso, newx = X[-ids,],
                          s = cv_lasso$lambda.1se) # valor predito fora da amostra

y_lasso_dentro2 <- predict(cv_lasso2, newx = X[ids,],
                          s = cv_lasso2$lambda.min) # valor predito dentro da amostra
y_lasso_dentro2.2 <- predict(cv_lasso2, newx = X[ids,],
                          s = cv_lasso2$lambda.1se) # valor predito dentro da amostra

y_lasso_fora2 <- predict(cv_lasso2, newx = X[-ids,],
                          s = cv_lasso2$lambda.min) # valor predito fora da amostra
y_lasso_fora2.2 <- predict(cv_lasso2, newx = X[-ids,],
                          s = cv_lasso2$lambda.1se) # valor predito fora da amostra


```

```{r}
erro_dentro_lasso <- mean(abs(y_lasso_dentro - treino_sf$Y[ids]))
erro_dentro_lasso1.2 <- mean(abs(y_lasso_dentro1.2 - treino_sf$Y[ids]))

erro_fora_lasso <- mean(abs((y_lasso_fora - treino_sf$Y[-ids])))
erro_fora_lasso1.2 <- mean(abs((y_lasso_fora1.2 - treino_sf$Y[-ids])))


erro_dentro_lasso2 <- mean(abs(y_lasso_dentro2 - treino_sf$Y[ids]))
erro_dentro_lasso2.2 <- mean(abs(y_lasso_dentro2.2 - treino_sf$Y[ids]))

erro_fora_lasso2 <- mean(abs((y_lasso_fora2 - treino_sf$Y[-ids])))
erro_fora_lasso2.2 <- mean(abs((y_lasso_fora2.2 - treino_sf$Y[-ids])))
```


```{r}
# resultados <- data.frame("lasso",lasso_erro_dentro, lasso_erro_fora)
# names(resultados) <- c("modelo", "erro_dentro", "erro_fora")

# Modelo 1
resultados <- rbind(resultados,data.frame(modelo="Lasso - lambda minimo",erro_dentro=erro_dentro_lasso, erro_fora=erro_fora_lasso))
resultados <- rbind(resultados,data.frame(modelo="Lasso - lambda 1 desvio padrao",erro_dentro=erro_dentro_lasso1.2, erro_fora=erro_fora_lasso1.2))

resultados <- rbind(resultados,data.frame(modelo="Lasso 2- lambda minimo",erro_dentro=erro_dentro_lasso2, erro_fora=erro_fora_lasso))
resultados <- rbind(resultados,data.frame(modelo="Lasso 2- lambda 1 desvio padrao",erro_dentro=erro_dentro_lasso2.2, erro_fora=erro_fora_lasso2.2))


resultados <- resultados %>% mutate(across(where(is.numeric),round,4))
resultados
```

Percebemos que em relação ao modelo média, há uma expressiva melhora tanto no erro dentro como no erro fora.  

Em relação ao uso do lambda mínimo ou lambda 1 desvio padrão, os resultados são muito semelhantes, sendo que o lambda mínimo obteve um desempenho um pouco melhor.

Em relação a aos modelos com a lista de lambdas fornecidas e com os 30 lambdas escolhidos pela função, os resultudados pbtidos foram muito semelhantes.

### Modelo de Floresta Aleatória

Foi escolhido o modelo de Floresta Aleatória já que dessa forma existe variabilidade na escolha das variáveis explicativas e na escolha das observações para a construção de cada árvore. Essa variabilidade por sua vez pode levar a uma melhor predição no conjunto de validação e no de teste.

Para realizar o ajuste do modelo, foi utilizado a função randomForest do pacote randomForest.

Como os nomes das estações são numéricas foi necessário fazer uma pequena alteração para que o comando randomForest funcionasse. 

```{r}
# library(randomForest) # funcoes para estimar floresta aleatoria
```


```{r}
# duplicar o bd e renomear as colunas para rodar as florestas aleatorias
treino_sf2 <- treino_sf
response_col <- which(colnames(treino_sf2) == "Y")
colnames(treino_sf2)[-response_col] <- paste0( "V", colnames(treino_sf2)[-response_col])
```


```{r, eval=FALSE}
modelo_rf <- randomForest(treino_sf2$Y[ids] ~ .,
                    data = treino_sf2[ids, ])
```


```{r, eval=FALSE}
saveRDS(modelo_rf, "sf_rf.rds")
```

```{r}
modelo_rf <- readRDS("sf_rf.rds")
```

A seguir, podemos ver as cinco estações mais importantes e as cinco menos importantes para a predição.

```{r}
# importancia das variaveis
a <- as.data.frame(importance(modelo_rf))
head(a %>% arrange(desc(IncNodePurity)),6)
tail(a %>% arrange(desc(IncNodePurity)),5)
rm(a)
```


Ainda, é possível vizualizar melhor no gráfico a seguir a importância de cada estação.

```{r}
varImpPlot(modelo_rf)
```

Percebemos claramente pelo gráfico que as estações 45298000,46105000,44290002,46998000,42210000,40025000 são as estações que mais contribuem para a previsão. As outras estações por outro lado contribuem pouco e de forma semelhante para a previsão.

```{r}
y_rf_dentro<- predict(modelo_rf, treino_sf2[ids,-1])
y_rf_fora<- predict(modelo_rf, treino_sf2[-ids,-1])
```

```{r}
erro_dentro_rf <- mean(abs(y_rf_dentro - treino_sf$Y[ids]))

erro_fora_rf <- mean(abs((y_rf_fora - treino_sf$Y[-ids])))
```
```{r}
# resultados <- data.frame("lasso",lasso_erro_dentro, lasso_erro_fora)
# names(resultados) <- c("modelo", "erro_dentro", "erro_fora")
# typeof(resultados[,2])
resultados <- rbind(resultados,data.frame(modelo="Floresta Aleatoria",erro_dentro=erro_dentro_rf, erro_fora=erro_fora_rf))
resultados <- resultados %>% mutate(across(where(is.numeric),round,4))
resultados
```

Novamente, percebemos uma melhora significativa em relação ao modelo média.

Em relação ao modelo de lasso, percebemos melhora expressiva no erro dentro e melhora significativa no erro fora.


### Redes neurais

Para o modelo de redes neurais, foi utilizada a função de ativação ReLU ao invés da sigmoid por ser computacionalmente mais rápida.

Vamos considerar um modelo com apenas uma camada oculta e 41 unidades, por corresponder a 50% da quantidade de camadas de entrada e um modelo com 3 camadas ocultas com 60,40,20 unidades respectivamente por corresponderem a aproximadamente 75%, 50%, 25%. 

Foram considerados 40% de supressão, 150 epochs e tamanho de batch = 32.



```{r, eval=TRUE, results=FALSE}
# Rede Neural -------------------------------------------------------------
# library(keras)

# primeiro passo: definir a estrutura que descreve a rede neural
# a ultima camada tem somente uma unidade

modelo_rn <- keras_model_sequential() %>%
  layer_dense(units = 41, # numero de unidades na camada oculta
              activation = "relu", # funcao de ativacao
              input_shape = ncol(X)) %>% # dimensao da entrada
  layer_dropout(rate = 0.4) %>% # taxa de nos nos suprime em cada etapa
  layer_dense(units = 1) # camadas de saida com uma unica unidade pq eh um modelo de regressao

modelo_rn
```


```{r, eval=TRUE}
# segundo passo: especificacoes que controlam o algoritmo de estimacao
modelo_rn %>%
  compile(loss = "mse",   # funcao de custo
          optimizer = optimizer_rmsprop(),  # otimizador
          metrics = list("mean_absolute_error"))   # metrica para avaliar o erro
# a funcao compile() nao muda a variavel R, mas
# comunica as especificacoes para a instancia
# python correspondente que foi criada
```

```{r, eval=FALSE}
# salvar o modelo
save_model_hdf5(modelo_rn, "sf_rn")
```

<!-- ```{r} -->
<!-- callback <- EarlyStopping( -->
<!--     monitor="val_loss", # erro a ser monitorado -->
<!--     min_delta=0, # diminuicao de erro minima -->
<!--     patience=10, # numero de vezes toleradas sem diminuicao do erro -->
<!--     verbose=0, -->
<!--     mode="auto", -->
<!--     baseline=None, -->
<!--     restore_best_weights=False, -->
<!-- ) -->

<!-- ``` -->



```{r, eval=TRUE, results=FALSE}
# terceiro passo: ajustar o modelo (estimar os parametros)
history <- modelo_rn %>%
  fit(X[ids,], # preditoras de treino    dados de entrada
      treino_sf$Y[ids],  # resposta de treino
      batch_size = 32, # quantas observacoes escolhidas aleatoriamentes
                       # em cada passo do SGD
      epochs = 150, # uma epoca e' numero de passos do SGD 1500
                     # para processar todos dados de treino
                     # nesse caso, cada epoca tem
                     # length(y[-ids_teste])/32 passos SGD
  #     callback = callback_early_stopping(monitor = "val_loss",
  # min_delta = 0,
  # patience = 20,
  # verbose = 0,
  # mode = c("auto"),
  # baseline = NULL,
  # restore_best_weights = FALSE),
      # dados de validacao para avaliar o progresso do modelo
      validation_data = list(X[-ids,],
                             treino_sf$Y[-ids]))
```


```{r, eval=FALSE}
saveRDS(history, "sf_history.rds")

```

```{r, eval=TRUE}
# o grafico abaixo mostra o MAE
# para os dados de treino e teste
plot(history)
# plot(history2,xlim(0,150))
```

```{r}
# predicao
y_rn_dentro <- predict(modelo_rn, X[ids, ])
y_rn_fora <- predict(modelo_rn, X[-ids, ])

```

Podemos observar os resultados a seguir.


```{r}
# calculo do erro absoluto medio
erro_dentro_rn <- mean(abs (treino_sf$Y[ids] - y_rn_dentro))
erro_fora_rn <- mean(abs (treino_sf$Y[-ids] - y_rn_fora))

resultados <- rbind(resultados,
                     data.frame(modelo = "Rede Neural",
                                erro_dentro = erro_dentro_rn,
                                erro_fora=erro_fora_rn))

```



```{r, eval=TRUE, results=FALSE}
# Rede Neural -------------------------------------------------------------
# library(keras)

# primeiro passo: definir a estrutura que descreve a rede neural
# a ultima camada tem somente uma unidade

modelo_rn2 <- keras_model_sequential() %>%
  layer_dense(units = 60, # numero de unidades na camada oculta
              activation = "relu", # funcao de ativacao
              input_shape = ncol(X)) %>% # dimensao da entrada
  layer_dropout(rate = 0.4) %>% # taxa de nos nos suprime em cada etapa
  layer_dense(units = 40, activation = 'relu') %>%
  layer_dropout(rate = 0.4) %>% # taxa de nos nos suprime em cada etapa
  layer_dense(units = 20, activation = 'relu') %>%
  layer_dropout(rate = 0.4) %>%
  layer_dense(units = 1) # camadas de saida com uma unica unidade pq eh um modelo de regressao

modelo_rn2
```


```{r, eval=TRUE}
# segundo passo: especificacoes que controlam o algoritmo de estimacao
modelo_rn2 %>%
  compile(loss = "mse",   # funcao de custo
          optimizer = optimizer_rmsprop(),  # otimizador
          metrics = list("mean_absolute_error"))   # metrica para avaliar o erro
# a funcao compile() nao muda a variavel R, mas
# comunica as especificacoes para a instancia
# python correspondente que foi criada
```

```{r, eval=TRUE}
# salvar o modelo
save_model_hdf5(modelo_rn2, "sf_rn2")
```

<!-- ```{r} -->
<!-- callback <- EarlyStopping( -->
<!--     monitor="val_loss", # erro a ser monitorado -->
<!--     min_delta=0, # diminuicao de erro minima -->
<!--     patience=10, # numero de vezes toleradas sem diminuicao do erro -->
<!--     verbose=0, -->
<!--     mode="auto", -->
<!--     baseline=None, -->
<!--     restore_best_weights=False, -->
<!-- ) -->

<!-- ``` -->



```{r, eval=TRUE, results=FALSE}
# terceiro passo: ajustar o modelo (estimar os parametros)
history2 <- modelo_rn2 %>%
  fit(X[ids,], # preditoras de treino    dados de entrada
      treino_sf$Y[ids],  # resposta de treino
      batch_size = 32, # quantas observacoes escolhidas aleatoriamentes
                       # em cada passo do SGD
      epochs = 150, # uma epoca e' numero de passos do SGD 1500
                     # para processar todos dados de treino
                     # nesse caso, cada epoca tem
                     # length(y[-ids_teste])/32 passos SGD
  #     callback = callback_early_stopping(monitor = "val_loss",
  # min_delta = 0,
  # patience = 20,
  # verbose = 0,
  # mode = c("auto"),
  # baseline = NULL,
  # restore_best_weights = FALSE),
      # dados de validacao para avaliar o progresso do modelo
      validation_data = list(X[-ids,],
                             treino_sf$Y[-ids]))
```


```{r, eval=TRUE}
saveRDS(history2, "sf_history2.rds")

```


```{r, eval=FALSE}
# carregar o modelo
modelo_rn <- load_model_hdf5("sf_rn")
history <- readRDS("sf_history.rds")

modelo_rn2 <- load_model_hdf5("sf_rn2")
history2 <- readRDS("sf_history2.rds")
```


```{r, eval=TRUE}
# o grafico abaixo mostra o MAE
# para os dados de treino e teste
# plot(history, xlim(0,150))
plot(history2,xlim(0,150))
```


```{r}
# predicao
y_rn_dentro2 <- predict(modelo_rn2, X[ids, ])
y_rn_fora2 <- predict(modelo_rn2, X[-ids, ])

```

Podemos observar os resultados a seguir.


```{r}
# calculo do erro absoluto medio

erro_dentro_rn2 <- mean(abs (treino_sf$Y[ids] - y_rn_dentro2))
erro_fora_rn2 <- mean(abs (treino_sf$Y[-ids] - y_rn_fora2))

(resultados <- rbind(resultados,
                     data.frame(modelo = "Rede Neural 2",
                                erro_dentro = erro_dentro_rn2,
                                erro_fora=erro_fora_rn2)))
```

Percebemos que aumentar o numero de camadas ocultas não melhorou o erro de previsão do modelo. Pelo contrário, piorou-a.

Dentre modelos, o que teve melhor desempenho tanto no erro dentro quanto no erro fora foi o modelo de floresta aleatória. Dessa forma, a previsão para os dados de teste foi feita utilizando o modelo de floresta aleatória.



<!-- ### Submissão dos resultados -->

<!-- O dataframe teste_sf está estruturado no mesmo formato que o dataframe treino_sf. A única diferença é que ele não tem a coluna Y, que é aquela que o modelo deve prever. -->

```{r}
# duplicar o bd e renomear as colunas para rodar as florestas aleatorias
teste_sf2 <- teste_sf
colnames(teste_sf2) <- paste0( "V", colnames(teste_sf2))

predicoes <- predict(modelo_rf, teste_sf2)
write.csv(data.frame(y_pred = predicoes), file="sf_9297784_yugooyama.csv")

```



<!-- ```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}  -->

<!-- ``` -->

  