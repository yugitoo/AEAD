---
title: "Trabalho de Aplicação"
author: 'Yugo Oyama NUSP: 9297784'
date: "14/11/2021"
output: pdf_document
---

```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Introdução

Este trabalho consiste na aplicação de técnicas estatísticas abordadas no curso de Estatística em Altas Dimensões - 2021 em dois bancos de dados que serão explicados detalhadamente mais a frente.

## Dados de digitos manuscritos


```{r, results=FALSE, warning=FALSE, message=FALSE}
library(dplyr)
library(glmnet) # tem as funcoes para lasso, ridge e elasticnet
library(keras)
library(randomForest)
```


<!-- ### Carregando os Bancos de dados -->

```{r}
load("dados_mnist.rdata")
load("dados_mnist_teste.rdata")
```

O banco de dados é composto por:  
x_treino: matriz com as 60000 imagens do conjunto de treino;  
y_treino: vetor com os reais valores dos dígitos escritos nas imagens do conjunto de treino (etiquetas);  

Cada linha da matriz x_treino é uma imagem. Aqui, cada imagem está representada como um vetor de dimensao 1x784. Cada coluna indica o tom de cinza do respectivo pixel da imagem (entre 0 - preto e 255 - branco).

<!-- Para visualizar estes dados como imagem, primeiro você deve transformar a linha com 784 colunas em uma matriz 28x28 e fazer a seguinte operação para rotacionar a figura: -->

<!-- ```{r} -->
<!-- matriz <- matrix(x_treino[1,], ncol = 28, byrow = TRUE) -->

<!-- imagem <- t(apply(matriz, 2, rev)) -->
<!-- ``` -->

<!-- Em seguida, para visualizá-la graficamente, utilize o código abaixo. -->

<!-- ```{r} -->
<!-- image(1:28, 1:28, imagem, col = gray((0:255)/255),  -->
<!--         xaxt = 'n', yaxt = 'n', xlab="", ylab="",  -->
<!--         main = (y_treino[1])) -->
<!-- ``` -->

<!-- O código e a figura abaixo mostram 36 figuras selecionadas aleatoriamente e suas etiquetas correspondentes. -->

<!-- ```{r} -->
<!-- par(mfcol = c(6,6)) -->
<!-- par(mar = c(0, 0, 2, 0), xaxs = 'i', yaxs = 'i')  -->

<!-- set.seed(12) -->

<!-- indices <- sample(nrow(x_treino), 36) -->
<!-- for (idx in indices) { -->
<!--   im <- matrix(x_treino[idx,], nrow=28, byrow = TRUE) -->
<!--   im <- t(apply(im, 2, rev)) -->
<!--   image(1:28, 1:28, im, col = gray((0:255)/255),  -->
<!--         xaxt = 'n', yaxt = 'n', xlab="", ylab="",  -->
<!--         main = (y_treino[idx])) -->
<!-- } -->
<!-- ``` -->

### Objetivo

O objetivo desta análise é propor um modelo que consiga realizar boas predições para a qual número corresponde cada uma das imagens.

### Técnicas

Para esse projeto serão usados modelos lineares com regularização, modelos baseados em árvores, e redes
neurais.

Para comparar os modelos entre si,será utilizada a acurácia e ao final de cada método testado, será adicionado o resultado a uma tabela comparativa.

Por motivos de tempo de processamento, apenas o modelo escolhido de cada categoria será apresentado no relatório.

### Parâmetros

Para o ajuste do modelo, foi definido que o conjunto de treino seria dividido em conjunto de treino e validação
na proporção 7,5:1,5, ou seja, 75% do conjunto originalmente de treino foi usado para validação. Foi definida
também uma semente por questão de reprodutibilidade (12345).

### Lasso

Para criar um modelo de regressão com a penalização lasso, foi utilizada a biblioteca glmnet. Com ela, dentro
da amostra de treino, foi ajustado o modelo de classificação com penalização lasso utizando validação cruzada.
Em seguida, foi testado o modelo obtido no conjunto de validação e calculado o respectivo erro dentro e fora

Para o ajuste do modelo, inicialmente testou-se os definir para a função os valores (0.01, 0.1, 1, 2, 10, 25, 50, 75, 100) e em seguida testou-se definir que o modelo escolhesse 30 lambdas diferentes. 

Existem dois lambdas que são popularmentes usados: o que minimiza o erro gerado pela validação cruzada e o no qual o erro não ultrapassa um desvio padrão do melhor modelo. Com isso, foram testados modelos com cada um dos lambdas e calculados os erros dentro e fora respectivos de cada um.

O modelo cujos valores de lambda foram escolhidos pelo cv.glmnet apresentou resultados significativamente melhores.

O cálculo de previsão foi realizado diretamente do modelo resultante da validação cruzada conforme recomendado na documentação do pacote glmnet por questão de convergência e otimização.

```{r}
# y_treino <- factor(y_treino)
db_digitos <- data.frame(y_treino=y_treino,x_treino)

set.seed(12345)
X <- model.matrix(db_digitos[,1] ~ .,
                  data = db_digitos[,-1])[,-1] # X deve ser uma matrix sem intercepto
```


```{r}
# separando 75% dos dados para treino
ids <- sample(nrow(db_digitos), size = .75*nrow(db_digitos), replace = FALSE)

```



```{r, eval=FALSE, results=FALSE}
cv_lasso <- cv.glmnet(X[ids,], as.factor(db_digitos$y_treino[ids]), family = "multinomial", alpha = 1,
                      type.measure = "class", trace.it = TRUE, nlambda = 30, maxit = 10000, nfolds = 10)

cv_lasso$lambda.min
cv_lasso$lambda.1se


# saveRDS(cv_lasso, "mnist_lasso2.rds")
```




```{r}
# lasso <- glmnet(X[ids,], as.factor(db_digitos$y_treino[ids]), alpha = 1, lambda = 0.05, family = "multinomial", intercept = FALSE)
```


```{r,eval=FALSE}
saveRDS(cv_lasso, "mnist_lasso.rds")
```

```{r, eval=TRUE}
cv_lasso <- readRDS("mnist_lasso.rds")
```



```{r, eval=FALSE}
cv_lasso <-readRDS("mnist_lasso.rds")

# cv_lasso2 <-readRDS("mnist_lasso2.rds")
```


```{r}
y_lasso_dentro <- predict(cv_lasso, newx = X[ids,], 
                          s = cv_lasso$lambda.min, type = "class") # valor predito dentro da amostra

y_lasso_dentro1.2 <- predict(cv_lasso, newx = X[ids,],
                          s = cv_lasso$lambda.1se, type = "class") # valor predito dentro da amostra

# y_lasso_dentro2 <- predict(cv_lasso2, newx = X[ids,],
#                           s = cv_lasso2$lambda.min, type = "class") # valor predito dentro da amostra
# 
# y_lasso_dentro2.2 <- predict(cv_lasso2, newx = X[ids,],
#                           s = cv_lasso2$lambda.1se, type = "class") # valor predito dentro da amostra

```


```{r, eval=FALSE}
# comparativo: observado vs predito (dentro da amostra)
(tabela_tr <- table(predito = y_lasso_dentro, observado = y_treino[ids]))

```

<!-- É possível perceber que os maiores erros ocorrem em -->
```{r}
#
y_lasso_fora <- predict(cv_lasso, newx = X[-ids,],
                        s = cv_lasso$lambda.min, type = "class") # valor predito fora da amostra

y_lasso_fora1.2 <- predict(cv_lasso, newx = X[-ids,],
                        s = cv_lasso$lambda.1se, type = "class") # valor 

# y_lasso_fora2 <- predict(cv_lasso, newx = X[-ids,],
#                         s = cv_lasso2$lambda.min, type = "class") # valor predito fora da amostra
# 
# y_lasso_fora2.2 <- predict(cv_lasso, newx = X[-ids,],
#                         s = cv_lasso2$lambda.1se, type = "class") # valor 

```


```{r}
lasso_ac_dentro<- mean(y_lasso_dentro==y_treino[ids])
lasso_ac_dentro1.2<- mean(y_lasso_dentro1.2==y_treino[ids])
lasso_ac_fora<- mean(y_lasso_fora==y_treino[-ids])
lasso_ac_fora1.2<- mean(y_lasso_fora1.2==y_treino[-ids])

# lasso_ac_dentro2<- mean(y_lasso_dentro2==y_treino[ids])
# lasso_ac_dentro2.2<- mean(y_lasso_dentro2.2==y_treino[ids])
# lasso_ac_fora2<- mean(y_lasso_fora2==y_treino[-ids])
# lasso_ac_fora2.2<- mean(y_lasso_fora2.2==y_treino[-ids])


```

```{r}
resultados <- data.frame("Lasso - Lambda minimo",lasso_ac_dentro, lasso_ac_fora)
names(resultados) <- c("modelo", "acuracia_dentro", "acuracia_fora")

resultados <- rbind(resultados,data.frame(modelo="Lasso - Lambda 1 desvio padrao",acuracia_dentro=lasso_ac_dentro1.2, acuracia_fora=lasso_ac_fora1.2))


# resultados <- rbind(resultados,data.frame(modelo="Lasso2 - Lambda minimo padrao",acuracia_dentro=lasso_ac_dentro2, acuracia_fora=lasso_ac_fora2))
# 
# resultados <- rbind(resultados,data.frame(modelo="Lasso2 - Lambda 1 desvio padrao",acuracia_dentro=lasso_ac_dentro2.2, acuracia_fora=lasso_ac_fora2.2))


resultados <- resultados %>% mutate(across(where(is.numeric),round,4))
resultados

```

Percebemos que usar o modelo com lambda mínimo ou o modelo com lambda 1 desvio padrão não resulta em muita diferença na acurárica.

O ajuste do modelo demorou algumas horas para ser realizado.

### Modelo de Árvores

Foi escolhido o modelo de Floresta Aleatória já que dessa forma existe variabilidade na escolha das variáveis explicativas e na escolha das observações para a construção de cada árvore. Essa variabilidade por sua vez pode levar a uma melhor predição no conjunto de validação e no de teste.

Para realizar o ajuste do modelo, foi utilizado a função randomForest do pacote randomForest.


```{r}
# library(tree) # funcoes para estimar arvore de reg/class
# library(randomForest) # funcoes para estimar floresta aleatoria

```


```{r, eval=FALSE}
modelo_rf <- randomForest(factor(db_digitos$y_treino[ids]) ~ .,
                    data = db_digitos[ids, ], do.trace=TRUE)
```

```{r, eval=FALSE}
saveRDS(modelo_rf, "mnist_rf.rds")
```

```{r}
modelo_rf <- readRDS("mnist_rf.rds")
```


<!-- ```{r} -->
<!-- # importancia das variaveis -->
<!-- importance(modelo_rf) -->
<!-- varImpPlot(modelo_rf) -->
<!-- ``` -->

A seguir, podemos ver os cinco pixels mais importantes e as cinco menos importantes para a predição.

```{r}
# importancia das variaveis
a <- as.data.frame(importance(modelo_rf))
head(a %>% arrange(desc(MeanDecreaseGini)),5)
tail(a %>% arrange(desc(MeanDecreaseGini)),5)
rm(a)
```

Percebemos, como esperado, que os pixels mais periféricos (mais próximos às bordas das imagens) são os que menos contribuem para o ajuste de previsão e os pixels mais centrais são os que mais contribuem.

Ainda, é possível vizualizar melhor no gráfico a seguir a importância de cada pixel.

```{r}
varImpPlot(modelo_rf)
```

Pelo gráfico, percebemos que o ajuste do modelo não é feito a partir de alguns pixels isolados e sim a partir, principalmente, do conjunto de pixels mais próximos do centro.

```{r}
y_rf_dentro<- predict(modelo_rf, db_digitos[ids,-1],type="class")
y_rf_fora<- predict(modelo_rf, db_digitos[-ids,-1],type="class")
```


```{r}
# acuracia
rf_ac_dentro <- mean(y_rf_dentro == db_digitos$y_treino[ids])
rf_ac_fora <- mean(y_rf_fora == db_digitos$y_treino[-ids])
```


```{r}

# resultados <- data.frame("Floresta Aleatoria",rf_ac_dentro, rf_ac_fora)
# names(resultados) <- c("modelo", "acuracia_dentro", "acuracia_fora")

# atualizacao do dataframe de resultados
resultados <- rbind(resultados,data.frame(modelo="Floresta aleatoria",acuracia_dentro=rf_ac_dentro, acuracia_fora=rf_ac_fora))
resultados <- resultados %>% mutate(across(where(is.numeric),round,4))
resultados

```

Percebemos que o modelo de floresta aleatória obteve acurácia maior que os modelos de regressão com penalidade Lasso tanto na acurácia dentro como na acurácia fora. 

Impressionantemente, a acurácia dentro foi de 100%, ou seja, não errou nenhuma predição.

### Redes neurais

Para o modelo de redes neurais, foi considerado um modelo com duas camadas ocultas com 256 e 128 unidades respectivamente com função de ativação ReLU.

Como temos mais de uma classificação, foi utilizada a função softmax como função de ativação na camada de saída.

Ainda, foi considerada supressão de 40%, 30 epochs e tamanho de batch 128 de forma a tentar otimizar o tempo de processamento do modelo.


```{r}
# library(keras)

X <- model.matrix(db_digitos[,1] ~ .,
                  data = db_digitos[,-1]) # X deve ser uma matrix

# reshape
# library(Corbi)
# x_train <- submatrix(x_treino,ids,1:784)
# x_test <- submatrix(x_treino,-ids,1:784)

x_train <- X[ids,]
x_test <- X[-ids,]

# rescale
x_train <- x_train / 255
x_test <- x_test / 255

```

```{r}
y_train <- to_categorical(y_treino[ids], 10)
y_test <- to_categorical(y_treino[-ids], 10)
```



```{r, eval=FALSE,results=FALSE}
# Rede Neural -------------------------------------------------------------
# library(keras)

# primeiro passo: definir a estrutura que descreve a rede neural
# a ultima camada tem somente uma unidade

modelo_rn <- keras_model_sequential()
modelo_rn %>%
  layer_dense(units = 256, # numero de unidades
              activation = "relu", # funcao de ativacao
              input_shape = ncol(x_train)) %>% # dimensao da entrada
  layer_dropout(rate = 0.4) %>% # taxa de nos nos suprime em cada etapa
  layer_dense(units = 128, activation = 'relu') %>%
  layer_dropout(rate = 0.4) %>%
  layer_dense(units = 10, # camadas de saida com uma unica unidade pq eh um modelo de classificacao com 10 categorias
              activation = "softmax") 

modelo_rn
```



```{r, eval=FALSE}
# segundo passo: especificacoes que controlam o algoritmo de estimacao
modelo_rn %>%
  compile(loss = "categorical_crossentropy",   # funcao de custo
          optimizer = optimizer_rmsprop(),  # otimizador
          metrics = c('accuracy'))   # metrica para avaliar o erro
# a funcao compile() nao muda a variavel R, mas
# comunica as especificacoes para a instancia
# python correspondente que foi criada
```


```{r, eval=FALSE}
# salvar o modelo
save_model_hdf5(modelo_rn, "mnist_rn")
```



```{r, eval=FALSE, results=FALSE}
# terceiro passo: ajustar o modelo (estimar os parametros)
history <- modelo_rn %>%
  fit(x_train, # preditoras de treino    dados de entrada
      y_train,  # resposta de treino
      batch_size = 128, # quantas observacoes escolhidas aleatoriamentes
                       # em cada passo do SGD
      epochs = 30, # uma epoca e' numero de passos do SGD 30
                     # para processar todos dados de treino
                     # nesse caso, cada epoca tem
                     # num de obs/batch passos pra completar uma epoca 
      verbose=1,
  #     callback = callback_early_stopping(monitor = "val_loss",
  # min_delta = 0,
  # patience = 20,
  # verbose = 0,
  # mode = c("auto"),
  # baseline = NULL,
  # restore_best_weights = FALSE),
      # dados de validacao para avaliar o progresso do modelo
      validation_data = list(x_test,
                             y_test))
```

```{r, eval=FALSE}
saveRDS(history, "mnist_history.rds")

```


```{r, eval=TRUE}
# carregar o modelo
modelo_rn <- load_model_hdf5("mnist_rn")
# history <- readRds("mnist_history.rds")
```


```{r}
rn_ac_dentro <- modelo_rn %>% evaluate(x_train, y_train)
rn_ac_fora <- modelo_rn %>% evaluate(x_test, y_test)
```


```{r}
# predicao

y_rn_dentro <- modelo_rn %>% predict(x_train) %>% k_argmax() %>% as.integer()
y_rn_fora <- modelo_rn %>% predict(x_test) %>% k_argmax() %>% as.integer()
```


<!-- ```{r} -->
<!-- # acuracia -->
<!-- rn_ac_dentro <- mean(y_rn_dentro == db_digitos$y_treino[ids]) -->
<!-- rn_ac_fora <- mean(y_rn_fora == db_digitos$y_treino[-ids]) -->
<!-- ``` -->


```{r}
# atualizacao do dataframe de resultados
resultados <- rbind(resultados,data.frame(modelo="Redes Neurais",acuracia_dentro=rn_ac_dentro[2], acuracia_fora=rn_ac_fora[2]))
resultados <- resultados %>% mutate(across(where(is.numeric),round,4))
resultados

```

O modelo de redes neurais também levou bem menos tempo que o modelo de regressão com penalidade lasso para ser ajustado. Ainda, obteve a maior acurácia fora (conjunto de validação) e acurácia bastante elevada dentro apesar de não tão alta como no modelo de floresta aleatória.

Como o objetivo desse projeto é predizer o número em um conjunto de dados nunca antes visto no modelo, a acurácia fora tem um peso maio que a dentro. Dessa forma, o modelo de redes neurais foi o escolhido para realizar as previsões do conjunto de teste.

Ainda, como os modelos testados já obtiveram acurácia bastante satisfatória, foi considerado que não havia necessidade de se testar mais modelos diferentes ou variações dos apresentados.

Para o conjunto de dados mnist, o modelo de redes neurais foi o melhor modelo, seguido pelo de floresta aleatória e por último o modelo de regressão logística com penalidade LASSO, sendo que o último não se mostrou um bom modelo tanto em tempo de processamento quanto em acurácia.

```{r, eval=FALSE}
predicoes <- modelo_rn %>% predict(x_teste) %>% k_argmax() %>% as.integer
```


```{r, eval=FALSE}
# Salva as predicoes em arquivo csv
write.csv(data.frame(y_pred = predicoes), file="mnist_9297784_yugooyama.csv")

```
